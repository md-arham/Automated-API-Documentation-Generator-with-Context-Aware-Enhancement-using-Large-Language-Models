{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14246436,"sourceType":"datasetVersion","datasetId":9089413},{"sourceId":14258798,"sourceType":"datasetVersion","datasetId":9098385},{"sourceId":14258834,"sourceType":"datasetVersion","datasetId":9098412}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-22T16:26:44.190033Z","iopub.execute_input":"2025-12-22T16:26:44.190326Z","iopub.status.idle":"2025-12-22T16:26:45.160903Z","shell.execute_reply.started":"2025-12-22T16:26:44.190292Z","shell.execute_reply":"2025-12-22T16:26:45.159974Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llamamodel3epoch/llamamodel/adapter_model.safetensors\n/kaggle/input/llamamodel3epoch/llamamodel/adapter_config.json\n/kaggle/input/llm-weights-3epoch/adapter_model.safetensors\n/kaggle/input/api-dataset/api_dataset/train.json\n/kaggle/input/api-dataset/api_dataset/test.json\n/kaggle/input/api-dataset/api_dataset/val.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# 1. Uninstall existing conflicting libraries\n!pip uninstall -y transformers peft trl bitsandbytes accelerate triton\n\n# 2. Install the \"Golden Combination\" for Kaggle T4\n!pip install -q -U \\\n  torch==2.3.0 \\\n  transformers==4.40.0 \\\n  peft==0.10.0 \\\n  trl==0.8.6 \\\n  accelerate==0.30.0 \\\n  bitsandbytes==0.42.0 \\\n  datasets==2.19.0\n  \n# Note: We skip installing 'triton' explicitly because bitsandbytes 0.42 \n# will use the built-in CUDA kernels instead of Triton if it's messy.\n# This prevents the 'triton.ops' error entirely.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T16:26:45.162415Z","iopub.execute_input":"2025-12-22T16:26:45.162901Z","iopub.status.idle":"2025-12-22T16:29:37.483074Z","shell.execute_reply.started":"2025-12-22T16:26:45.162870Z","shell.execute_reply":"2025-12-22T16:29:37.482125Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.57.1\nUninstalling transformers-4.57.1:\n  Successfully uninstalled transformers-4.57.1\nFound existing installation: peft 0.17.1\nUninstalling peft-0.17.1:\n  Successfully uninstalled peft-0.17.1\n\u001b[33mWARNING: Skipping trl as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: accelerate 1.11.0\nUninstalling accelerate-1.11.0:\n  Successfully uninstalled accelerate-1.11.0\nFound existing installation: triton 3.4.0\nUninstalling triton-3.4.0:\n  Successfully uninstalled triton-3.4.0\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m240.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntorchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.3.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\ntorchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install evaluate rouge_score nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T16:29:37.484582Z","iopub.execute_input":"2025-12-22T16:29:37.484896Z","iopub.status.idle":"2025-12-22T16:29:43.931745Z","shell.execute_reply.started":"2025-12-22T16:29:37.484867Z","shell.execute_reply":"2025-12-22T16:29:43.930858Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.2)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.19.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\nRequirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\nRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.7)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.13.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.22.0)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=caa627fa0275d9b24859a8eeb40d1189c842125fe74e899a66106b5e72e2aa0d\n  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score, evaluate\nSuccessfully installed evaluate-0.4.6 rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    BitsAndBytesConfig, \n    TrainingArguments,\n    pipeline\n)\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\nfrom trl import SFTTrainer\nimport evaluate\nimport nltk\n\n# --- CONFIGURATION ---\nMODEL_ID = \"unsloth/llama-3-8b-Instruct-bnb-4bit\" # Pre-quantized base model\nOUTPUT_DIR = \"llama3-openapi-adapter\"\nMAX_SEQ_LENGTH = 512  # Increased to 512 to be safe for longer docs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T16:29:43.933034Z","iopub.execute_input":"2025-12-22T16:29:43.933270Z","iopub.status.idle":"2025-12-22T16:30:04.741210Z","shell.execute_reply.started":"2025-12-22T16:29:43.933246Z","shell.execute_reply":"2025-12-22T16:30:04.740594Z"}},"outputs":[{"name":"stderr","text":"2025-12-22 16:29:53.263957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766420993.453653      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766420993.508200      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766420993.959690      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766420993.959739      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766420993.959742      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766420993.959745      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Load local JSON files into HuggingFace Dataset format\ndataset = load_dataset(\"json\", data_files={\n    \"train\": \"/kaggle/input/api-dataset/api_dataset/train.json\",\n    \"validation\": \"/kaggle/input/api-dataset/api_dataset/val.json\",\n    \"test\": \"/kaggle/input/api-dataset/api_dataset/test.json\"\n})\n\nprint(\"âœ… Data Loaded:\")\nprint(f\"   Train: {len(dataset['train'])} examples\")\nprint(f\"   Val:   {len(dataset['validation'])} examples\")\nprint(f\"   Test:  {len(dataset['test'])} examples\")\n\n# Example check\nprint(\"\\nğŸ” Sample Input:\")\nprint(dataset['train'][0]['input_text'])\nprint(\"ğŸ” Sample Target:\")\nprint(dataset['train'][0]['target_text'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T16:30:04.742657Z","iopub.execute_input":"2025-12-22T16:30:04.743192Z","iopub.status.idle":"2025-12-22T16:30:05.157622Z","shell.execute_reply.started":"2025-12-22T16:30:04.743167Z","shell.execute_reply":"2025-12-22T16:30:05.156988Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55bb642dc7e7493591693efd296f41ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc460e3d8304911b515a25507b6ec51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eea48319d60c42afa30fc321b0d2ba42"}},"metadata":{}},{"name":"stdout","text":"âœ… Data Loaded:\n   Train: 6341 examples\n   Val:   793 examples\n   Test:  793 examples\n\nğŸ” Sample Input:\nMethod: GET | Path: /V1/companyCredits/company/{companyId} | Summary: companyCredits/company/{companyId} | Tags: companyCredits/company/{companyId}\nğŸ” Sample Target:\nReturns data on the credit limit for a specified company.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# 4-bit Quantization Config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\ntokenizer.pad_token = tokenizer.eos_token # Fix for Llama-3 padding\ntokenizer.padding_side = \"right\" # Fix for FP16 training\n\n# Load Model\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    use_cache=False # Disable cache during training\n)\nmodel.config.pretraining_tp = 1 \n\nprint(\"âœ… Model Loaded in 4-bit!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T04:57:13.851239Z","iopub.execute_input":"2025-12-22T04:57:13.852314Z","iopub.status.idle":"2025-12-22T04:57:57.634422Z","shell.execute_reply.started":"2025-12-22T04:57:13.852276Z","shell.execute_reply":"2025-12-22T04:57:57.633723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport torch\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T05:03:42.657409Z","iopub.execute_input":"2025-12-22T05:03:42.657788Z","iopub.status.idle":"2025-12-22T05:03:43.295676Z","shell.execute_reply.started":"2025-12-22T05:03:42.657754Z","shell.execute_reply":"2025-12-22T05:03:43.295021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. LoRA CONFIGURATION ---\npeft_config = LoraConfig(\n    r=8,        \n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n)\n\n# --- 2. FORMATTING FUNCTION ---\n# Converts your input/target pairs into the Llama-3 chat format\ndef formatting_prompts_func(examples):\n    texts = []\n    for input_text, target_text in zip(examples['input_text'], examples['target_text']):\n        # Llama-3 Prompt Template\n        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are an expert technical writer. Write a concise, professional API description.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{target_text}<|eot_id|>\"\"\"\n        texts.append(prompt)\n    return texts\n\n# --- 3. MEMORY-OPTIMIZED TRAINING ARGUMENTS ---\nargs = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=2,  # REDUCED: 4 -> 2 (Critical for T4)\n    gradient_accumulation_steps=16,  # INCREASED: 4 -> 8 (Keeps effective batch size = 16)\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,\n    logging_steps=25,\n    save_strategy=\"epoch\",\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    report_to=\"none\",\n    \n    # --- CRITICAL MEMORY FIXES ---\n    gradient_checkpointing=True,    # SAVES ~50% VRAM (Trades speed for memory)\n    optim=\"paged_adamw_8bit\",       # SAVES VRAM (Pager optimizer moves states to CPU if needed)\n    max_grad_norm=0.3,              # Prevents gradient explosions consuming memory\n    warmup_ratio=0.03,\n    group_by_length=True,           # Groups similar length prompts (saves padding memory)\n)\n\n# --- 4. TRAINER WITH GRADIENT CHECKPOINTING ---\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"validation\"],\n    peft_config=peft_config,\n    formatting_func=formatting_prompts_func,\n    max_seq_length=256,\n    tokenizer=tokenizer,\n    args=args,\n    packing=False\n)\n\n# Enable gradient checkpointing explicitly on the model\nmodel.config.use_cache = False  # Silence warnings during training\ntrainer.model.print_trainable_parameters() \n\nprint(\"ğŸš€ Starting MEMORY-SAFE QLoRA Fine-Tuning...\")\ntrainer.train()\n\n# Save final adapter\ntrainer.model.save_pretrained(\"final_lora_adapter\")\nprint(\"âœ… Adapter Saved to 'final_lora_adapter'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T05:04:12.93879Z","iopub.execute_input":"2025-12-22T05:04:12.939114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Load ROUGE\nrouge = evaluate.load(\"rouge\")\n\ndef generate_response(input_text):\n    # Format input exactly like training\n    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are an expert technical writer. Write a concise, professional API description.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs, \n            max_new_tokens=128, \n            pad_token_id=tokenizer.eos_token_id,\n            do_sample=False # Greedy decoding for reproducibility\n        )\n    \n    # Extract only the response (remove the prompt)\n    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n    return response.strip()\n\n# --- RUN EVALUATION ON TEST SET (Sample 100 for speed, or all for accuracy) ---\nprint(\"ğŸš€ Generating predictions on Test Set...\")\npredictions = []\nreferences = []\n\n# Using first 100 examples for quick feedback (run full set for final report)\ntest_subset = dataset[\"test\"].select(range(100)) \n\nfor row in tqdm(test_subset):\n    pred = generate_response(row[\"input_text\"])\n    predictions.append(pred)\n    references.append(row[\"target_text\"])\n\n# Compute ROUGE\nprint(\"\\nğŸ“Š Calculating Metrics...\")\nresults = rouge.compute(predictions=predictions, references=references)\nprint(\"ğŸ† QLoRA Results:\", results)\n\n# Show examples\nprint(\"\\nğŸ” Example Output:\")\nprint(f\"Target: {references[0]}\")\nprint(f\"Predicted: {predictions[0]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"evaluation in new way","metadata":{}},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport evaluate\nfrom tqdm import tqdm\n\n# --- 1. LOAD MODEL (If not already loaded) ---\n# If your model is already in memory from training, skip this block.\n# If you restarted, run this:\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n# Load your found weights\n# pointing to the folder where adapter_model.bin or safetensors is\nmodel = PeftModel.from_pretrained(base_model, \"/kaggle/input/llamamodel3epoch/llamamodel\") \nmodel.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(\"unsloth/llama-3-8b-Instruct-bnb-4bit\")\ntokenizer.pad_token = tokenizer.eos_token\n\n# --- 2. OPTIMIZED GENERATION FUNCTION ---\nrouge = evaluate.load(\"rouge\")\n\ndef generate_response(input_text):\n    # Prompt: We explicitly ask for brevity here too\n    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nWrite a single-sentence description for this API.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs, \n            # --- CRITICAL PARAMETERS FOR BREVITY ---\n            max_new_tokens=64,        # Force stop after ~40-50 words\n            min_length=10,            # Ensure it generates *something*\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n            do_sample=False,          # Greedy decoding (most focused)\n            repetition_penalty=1.2,   # Punish it for repeating \"Reviews are...\"\n            temperature=0.1,          # Low randomness = more concise\n            # Stop tokens (optional): Force stop at newline if needed\n            # stopping_criteria=... \n        )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n    \n    # Post-processing: Cut off if it starts a new bullet point or paragraph\n    if \"\\n\" in response:\n        response = response.split(\"\\n\")[0]\n        \n    return response.strip()\n\n# --- 3. RUN EVALUATION ---\nprint(\"ğŸš€ Generating Optimized Predictions...\")\npredictions = []\nreferences = []\n\n# Test on 200 samples (enough for a good estimate)\ntest_subset = dataset[\"test\"].select(range(200)) \n\nfor row in tqdm(test_subset):\n    pred = generate_response(row[\"input_text\"])\n    predictions.append(pred)\n    references.append(row[\"target_text\"])\n\n# Compute ROUGE\nprint(\"\\nğŸ“Š Calculating Metrics...\")\nresults = rouge.compute(predictions=predictions, references=references)\nprint(\"ğŸ† QLoRA Results (Optimized):\", results)\n\n# Check the first few\nprint(\"\\nğŸ” Example Output:\")\nprint(f\"Target: {references[0]}\")\nprint(f\"Predicted: {predictions[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T16:31:10.602859Z","iopub.execute_input":"2025-12-22T16:31:10.603163Z","iopub.status.idle":"2025-12-22T17:02:04.286639Z","shell.execute_reply.started":"2025-12-22T16:31:10.603136Z","shell.execute_reply":"2025-12-22T17:02:04.285882Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55b19e1469c34633b2aadcd0495a7345"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17ce43d0cfab49369ebf2ed290f736e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9de7bbfb7e7647cd898ec1dac1ebfc11"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4ffba58354a47189efc3c5d9c60adce"}},"metadata":{}},{"name":"stdout","text":"ğŸš€ Generating Optimized Predictions...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [30:43<00:00,  9.22s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ“Š Calculating Metrics...\nğŸ† QLoRA Results (Optimized): {'rouge1': np.float64(0.3444165847872224), 'rouge2': np.float64(0.2227221811979705), 'rougeL': np.float64(0.29737349321424084), 'rougeLsum': np.float64(0.2967377970714394)}\n\nğŸ” Example Output:\nTarget: Create a review for the individual watchlist screening. Reviews are compliance reports created by users in your organization regarding the relevance of potential hits found by Plaid.\nPredicted: Create a review of the results returned by Plaid, and optionally add notes about your decision to allow or reject the customer. Reviews are optional but highly recommended as they provide audit trail information in case you need it later. If reviews are not created, all screenings will be automatically approved after 30 days. Note that if\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\n\n# --- 1. SETUP METRICS ---\nrouge = evaluate.load(\"rouge\")\nbleu = evaluate.load(\"bleu\")\nbertscore = evaluate.load(\"bertscore\")\n\nprint(\"âœ… Metrics Loaded (ROUGE, BLEU, BERTScore)\")\n\n# --- 2. GENERATE PREDICTIONS (Using your 200 sample subset) ---\n# Assuming 'predictions' and 'references' lists are already populated \n# from your previous run (the one that gave you 34.4%). \n# If not, re-run the generation loop first!\n\n# --- 3. CALCULATE ALL METRICS ---\nprint(\"ğŸ“Š Calculating ALL Metrics...\")\n\n# A. ROUGE\nrouge_results = rouge.compute(predictions=predictions, references=references)\n\n# B. BLEU\n# BLEU expects references as list of lists: [['ref']]\nbleu_refs = [[r] for r in references]\nbleu_results = bleu.compute(predictions=predictions, references=bleu_refs)\n\n# C. BERTScore (The semantic tie-breaker!)\n# lang=\"en\" downloads a roberta-large model (~1.4GB) - might take a minute\nbert_results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\nbert_f1 = np.mean(bert_results[\"f1\"]) # Average F1 across all samples\n\n# --- 4. PRINT FINAL TABLE ---\nprint(\"\\n\" + \"=\"*40)\nprint(\"ğŸ† FINAL QLoRA EVALUATION RESULTS\")\nprint(\"=\"*40)\nprint(f\"ROUGE-1:      {rouge_results['rouge1']*100:.2f}\")\nprint(f\"ROUGE-2:      {rouge_results['rouge2']*100:.2f}\")\nprint(f\"ROUGE-L:      {rouge_results['rougeL']*100:.2f}\")\nprint(\"-\" * 40)\nprint(f\"BLEU:         {bleu_results['bleu']*100:.2f}\")\nprint(\"-\" * 40)\nprint(f\"BERTScore F1: {bert_f1*100:.2f}\")  # This should be high (85%+)\nprint(\"=\"*40)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T17:29:41.984683Z","iopub.execute_input":"2025-12-22T17:29:41.985429Z","iopub.status.idle":"2025-12-22T17:30:07.609312Z","shell.execute_reply.started":"2025-12-22T17:29:41.985396Z","shell.execute_reply":"2025-12-22T17:30:07.608600Z"}},"outputs":[{"name":"stdout","text":"âœ… Metrics Loaded (ROUGE, BLEU, BERTScore)\nğŸ“Š Calculating ALL Metrics...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f21e9a4cc76d40e8a4e2df31864c1adf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4168512953264d21b9f1f0b0342ba61e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e74a7af2524e4bd2afc06879910052ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4863a153e7af4309ad5f0935d8610a64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4ccb7f2a83d43c9a52e9bccd4257341"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea7b8be23f1d4967bf0a5e8180799008"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n========================================\nğŸ† FINAL QLoRA EVALUATION RESULTS\n========================================\nROUGE-1:      34.44\nROUGE-2:      22.27\nROUGE-L:      29.74\n----------------------------------------\nBLEU:         22.34\n----------------------------------------\nBERTScore F1: 87.54\n========================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}